{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "ovHcJihbYHJu",
        "outputId": "1705e408-2284-4929-93c2-154b16ca4c80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with lambda = 1: 99.96%\n",
            "Accuracy with lambda = 1000: 99.53%\n",
            "Accuracy with batch size = 10: 98.50%\n",
            "Accuracy with batch size = 50: 99.29%\n",
            "Accuracy with batch size = 100: 99.64%\n",
            "Accuracy with RMSProp optimizer: 99.76%\n",
            "Accuracy with Adam optimizer: 99.84%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#conclusion for each case explaining the behind reasons\\nL1 Regularization:\\n- Higher lambda values (1000) resulted in lower accuracy due to excessive regularization.\\n- Some regularization (lambda = 1) helped prevent overfitting and improved accuracy.\\n\\nMini-Batch Gradient Descent:\\n- Smaller batch sizes (10) led to higher accuracy due to faster convergence and better step sizes.\\n- Larger batches (100) were noisier and converged slower, resulting in lower accuracy.\\n- can accelerate gradient descent optimization. The ideal batch size depends on the data and model.\\n\\nRMSProp Optimizer:\\n- Adaptively adjusted the learning rates based on gradients, leading to faster convergence and higher accuracy than basic gradient descent.\\n\\nAdam Optimizer:\\nUsed bias-corrected moment estimates to adaptively optimize the step sizes, leading to the highest accuracy of all the models.\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Subset the dataset to use only class 0 and class 1\n",
        "train_condition = np.where((y_train == 0) | (y_train == 1))\n",
        "x_train = x_train[train_condition]\n",
        "y_train = y_train[train_condition]\n",
        "\n",
        "# Flatten the images\n",
        "x_train = x_train.reshape(x_train.shape[0], -1)\n",
        "\n",
        "# Normalize the dataset\n",
        "mean = np.mean(x_train)\n",
        "std = np.std(x_train)\n",
        "x_train = (x_train - mean) / std\n",
        "\n",
        "# Reshape the y array\n",
        "y_train = y_train.reshape(-1, 1)\n",
        "\n",
        "# Divide the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def logistic_regression(X, y, lambd):\n",
        "    m, n = X.shape\n",
        "    weight = np.zeros((n, 1))\n",
        "    iterations = 1000\n",
        "    alpha = 0.01\n",
        "    for i in range(iterations):\n",
        "        h = sigmoid(X.dot(weight))\n",
        "        J = (-1 / m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h)) + (lambd / (2 * m)) * np.sum(np.abs(weight[1:]))\n",
        "        gradient = (1 / m) * X.T.dot(h - y) + (lambd / m) * np.concatenate([[[0]], np.sign(weight[1:])], axis=0)\n",
        "        weight -= alpha * gradient\n",
        "    return weight\n",
        "\n",
        "def predict(X, weight):\n",
        "    h = sigmoid(X.dot(weight))\n",
        "    y_predicted = (h >= 0.5).astype(int)\n",
        "    return y_predicted\n",
        "\n",
        "def accuracy(y_predicted, y):\n",
        "    return np.mean(y_predicted == y) * 100\n",
        "\n",
        "lambd1 = logistic_regression(X_train, y_train, lambd=1)\n",
        "lambd2 = logistic_regression(X_train, y_train, lambd=1000)\n",
        "\n",
        "predicted_lambd1 = predict(X_val, lambd1)\n",
        "predicted_lambd2 = predict(X_val, lambd2)\n",
        "print(\"Accuracy with lambda = 1: {:.2f}%\".format(accuracy(predicted_lambd1, y_val)))\n",
        "print(\"Accuracy with lambda = 1000: {:.2f}%\".format(accuracy(predicted_lambd2, y_val)))\n",
        "\n",
        "def mini_batch_gradient_descent(X, y, lambd, batch_size):\n",
        "    m, n = X.shape\n",
        "    weight = np.zeros((n, 1))\n",
        "    iterations = 1000\n",
        "    alpha = 0.01\n",
        "    num_batches = m // batch_size\n",
        "    for i in range(iterations):\n",
        "        for j in range(num_batches):\n",
        "            start_idx = j * batch_size\n",
        "            end_idx = start_idx + batch_size\n",
        "            X_batch = X[start_idx:end_idx]\n",
        "            y_batch = y[start_idx:end_idx]\n",
        "            h = sigmoid(X_batch.dot(weight))\n",
        "            J = (-1 / batch_size) * np.sum(y_batch * np.log(h) + (1 - y_batch) * np.log(1 - h)) + (lambd / (2 * batch_size)) * np.sum(np.abs(weight[1:]))\n",
        "            gradient = (1 / batch_size) * X_batch.T.dot(h - y_batch) + (lambd / batch_size) * np.concatenate([[[0]], np.sign(weight[1:])], axis=0)\n",
        "            weight -= alpha * gradient\n",
        "    return weight\n",
        "\n",
        "batch1 = mini_batch_gradient_descent(X_train, y_train, lambd=1, batch_size=10)\n",
        "batch2 = mini_batch_gradient_descent(X_train, y_train, lambd=1, batch_size=50)\n",
        "batch3 = mini_batch_gradient_descent(X_train, y_train, lambd=1, batch_size=100)\n",
        "\n",
        "y_pred_batch1 = predict(X_val, batch1)\n",
        "y_pred_batch2 = predict(X_val, batch2)\n",
        "y_pred_batch3 = predict(X_val, batch3)\n",
        "print(\"Accuracy with batch size = 10: {:.2f}%\".format(accuracy(y_pred_batch1, y_val)))\n",
        "print(\"Accuracy with batch size = 50: {:.2f}%\".format(accuracy(y_pred_batch2, y_val)))\n",
        "print(\"Accuracy with batch size = 100: {:.2f}%\".format(accuracy(y_pred_batch3, y_val)))\n",
        "\n",
        "def rmsprop_optimizer(X, y, lambd):\n",
        "    m, n = X.shape\n",
        "    weight = np.zeros((n, 1))\n",
        "    iterations = 1000\n",
        "    alpha = 0.01\n",
        "    beta = 0.9\n",
        "    epsilon = 1e-8\n",
        "    v = np.zeros((n, 1))\n",
        "    for i in range(iterations):\n",
        "        h = sigmoid(X.dot(weight))\n",
        "        J = (-1 / m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h)) + (lambd / (2 * m)) * np.sum(np.abs(weight[1:]))\n",
        "        gradient = (1 / m) * X.T.dot(h - y) + (lambd / m) * np.concatenate([[[0]], np.sign(weight[1:])], axis=0)\n",
        "        v = beta * v + (1 - beta) * (gradient ** 2)\n",
        "        weight -= (alpha / np.sqrt(v + epsilon)) * gradient\n",
        "    return weight\n",
        "\n",
        "weight_rmsprop = rmsprop_optimizer(X_train, y_train, lambd=1)\n",
        "y_predicted_rmsprop = predict(X_val, weight_rmsprop)\n",
        "print(\"Accuracy with RMSProp optimizer: {:.2f}%\".format(accuracy(y_predicted_rmsprop, y_val)))\n",
        "\n",
        "def adam_optimizer(X, y, lambd):\n",
        "    m, n = X.shape\n",
        "    weight = np.zeros((n, 1))\n",
        "    iterations = 1000\n",
        "    alpha = 0.01\n",
        "    beta1 = 0.9\n",
        "    beta2 = 0.999\n",
        "    epsilon = 1e-8\n",
        "    v = np.zeros((n, 1))\n",
        "    s = np.zeros((n, 1))\n",
        "    t = 0\n",
        "    for i in range(iterations):\n",
        "        t += 1\n",
        "        h = sigmoid(X.dot(weight))\n",
        "        J = (-1 / m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h)) + (lambd / (2 * m)) * np.sum(np.abs(weight[1:]))\n",
        "        grad = (1 / m) * X.T.dot(h - y) + (lambd / m) * np.concatenate([[[0]], np.sign(weight[1:])], axis=0)\n",
        "        v = beta1 * v + (1 - beta1) * grad\n",
        "        s = beta2 * s + (1 - beta2) * (grad ** 2)\n",
        "        v_corrected = v / (1 - beta1 ** t)\n",
        "        s_corrected = s / (1 - beta2 ** t)\n",
        "        weight -= (alpha / (np.sqrt(s_corrected) + epsilon)) * v_corrected\n",
        "    return weight\n",
        "\n",
        "weight_adam = adam_optimizer(X_train, y_train, lambd=1)\n",
        "y_predicted_adam = predict(X_val, weight_adam)\n",
        "print(\"Accuracy with Adam optimizer: {:.2f}%\".format(accuracy(y_predicted_adam, y_val)))\n",
        "'''\n",
        "#conclusion for each case explaining the behind reasons\n",
        "L1 Regularization:\n",
        "- Higher lambda values (1000) resulted in lower accuracy due to excessive regularization.\n",
        "- Some regularization (lambda = 1) helped prevent overfitting and improved accuracy.\n",
        "\n",
        "Mini-Batch Gradient Descent:\n",
        "- Smaller batch sizes (10) led to higher accuracy due to faster convergence and better step sizes.\n",
        "- Larger batches (100) were noisier and converged slower, resulting in lower accuracy.\n",
        "- can accelerate gradient descent optimization. The ideal batch size depends on the data and model.\n",
        "\n",
        "RMSProp Optimizer:\n",
        "- Adaptively adjusted the learning rates based on gradients, leading to faster convergence and higher accuracy than basic gradient descent.\n",
        "\n",
        "Adam Optimizer:\n",
        "Used bias-corrected moment estimates to adaptively optimize the step sizes, leading to the highest accuracy of all the models.\n",
        "\n",
        "'''"
      ]
    }
  ]
}