# -*- coding: utf-8 -*-
"""Assignment2_20200373_20200108.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AwtIkXGk05rQ864uS6OeaDZvafnUxsyp
"""

import numpy as np
from keras.datasets import mnist
from sklearn.model_selection import train_test_split

# Load the MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Subset the dataset to use only class 0 and class 1
train_condition = np.where((y_train == 0) | (y_train == 1))
x_train = x_train[train_condition]
y_train = y_train[train_condition]

# Flatten the images
x_train = x_train.reshape(x_train.shape[0], -1)

# Normalize the dataset
mean = np.mean(x_train)
std = np.std(x_train)
x_train = (x_train - mean) / std

# Reshape the y array
y_train = y_train.reshape(-1, 1)

# Divide the dataset into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)

# Define the logistic regression function using the sigmoid function and the L1 regularization
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

#Sparsity means that many of the weights are set to zero, which results in a simpler and more interpretable model that is less prone to overfitting
##encourages the model to have sparse weights by adding the sum of absolute values of the weights to the cost function.
def logistic_regression(X, y, lambd):  #X: A matrix of shape (m, n), m is the number of examples,n is the number of features. // y: A column vector of shape (m, 1) containing the labels for the training examples.  //lambd: A regularization hyperparameter that controls the strength of the L1 regularization.
    m, n = X.shape
    weight = np.zeros((n, 1))# weight // initializes a column vector of shape (n, 1) with all elements set to zero.
    #performs gradient descent to minimize the cost function
    iterations = 1000
    alpha = 0.01
    #The loop iteratively updates the weights
    for i in range(iterations):
        h = sigmoid(X.dot(weight))  # x.w  assuming that the input data X has already been augmented with a column of ones(b). // z = w_0 * 1(b) + w_1 * x_1 + w_2 * x_2 + ... + w_n * x_n
        J = (-1 / m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h)) + (lambd / (2 * m)) * np.sum(np.abs(weight[1:])) #we select all the elements of w except the first one(b)because it is not subject to regularization and does not contribute to sparsity.
        gradient = (1 / m) * X.T.dot(h - y) + (lambd / m) * np.concatenate([[[0]], np.sign(weight[1:])], axis=0)#The transpose allows us to perform matrix multiplication between the error vector (h - y) and the feature matrix X
        #[0]adds a zero to the gradient of the (b) to prevent it from being regularized,and concatenates it with the gradient of the other weights that have been modified by the sign function.
        #(sign)the sign function is applied to the weights to obtain a new vector of weights that has the same signs as the original vector but with magnitudes of 1
        # axis=0, the arrays are stacked vertically
        #all results in a vector of the same shape as theta
        weight -= alpha * gradient
    return weight

# Define the predict function
#used to evaluate the performance of a logistic regression model on a test set
#predicts the binary labels using the weights learned during training, and compares the predicted labels to the true labels to compute the accuracy.
def predict(X, weight):
    h = sigmoid(X.dot(weight))
    #Convert the probabilities to binary labels by setting a threshold of 0.5. If the probability is greater than or equal to 0.5, the label is set to 1, otherwise it is set to 0
    y_predicted = (h >= 0.5).astype(int)# astype(int) method can be used to convert the data type of the target variable from a float to an integer.
    return y_predicted

# Calculate the accuracy
def accuracy(y_predicted, y):
    return np.mean(y_predicted == y) * 100 #Compute the mean of the Boolean vector(same length as y_predicted and y)(the Boolean vector represents the proportion of correct predictions)

# Train the logistic regression model using gradient descent with L1 regularization for two different values of lambda
lambd1 = logistic_regression(X_train, y_train, lambd=1)
lambd2 = logistic_regression(X_train, y_train, lambd=1000)

# Calculate the accuracy on the validation set
predicted_lambd1 = predict(X_val, lambd1)
predicted_lambd2 = predict(X_val,lambd2)
print("Accuracy with lambda = 1 : {:.2f}%".format(accuracy(predicted_lambd1, y_val))) #{:.2f}%" is used to format the value with two decimal places followed by a percentage sign (99.67 -> 99.67%)
print("Accuracy with lambda = 2 : {:.2f}%".format(accuracy(predicted_lambd2, y_val)))


def mini_batch_gradient_descent(X, y, lambd, batch_size):
    m, n = X.shape
    weight = np.zeros((n, 1))
    iterations = 1000
    alpha = 0.01
    num_batches = m // batch_size # // perform integer division
    for i in range(iterations):
        for j in range(num_batches): #j is the current mini-batch number, which ranges from 0 to num_batches - 1
            start_idx = j * batch_size
            end_idx = start_idx + batch_size
            #extract the mini-batch of training examples and labels from the full training set using the start and end indices
            X_batch = X[start_idx:end_idx]
            y_batch = y[start_idx:end_idx]
            h = sigmoid(X_batch.dot(weight))
            J = (-1 / batch_size) * np.sum(y_batch * np.log(h) + (1 - y_batch) * np.log(1 - h)) + (lambd / (2 * batch_size)) * np.sum(np.abs(weight[1:]))
            gradient = (1 / batch_size) * X_batch.T.dot(h - y_batch) + (lambd / batch_size) * np.concatenate([[[0]], np.sign(weight[1:])], axis=0)
            weight -= alpha * gradient
    return weight

#Train the logistic regression model using mini batch gradient descent with three different batches
batch1 = mini_batch_gradient_descent(X_train, y_train, lambd=1, batch_size=10)
batch2 = mini_batch_gradient_descent(X_train, y_train, lambd=1, batch_size=50)
batch3 = mini_batch_gradient_descent(X_train, y_train, lambd=1, batch_size=100)

# Calculate the accuracy on the validation set
y_pred_batch1 = predict(X_val,batch1)
y_pred_batch2 = predict(X_val,batch2)
y_pred_batch3 = predict(X_val,batch3)
print("Accuracy with batch size = 1: {:.2f}%".format(accuracy(y_pred_batch1, y_val)))
print("Accuracy with batch size = 2: {:.2f}%".format(accuracy(y_pred_batch2, y_val)))
print("Accuracy with batch size = 3: {:.2f}%".format(accuracy(y_pred_batch3, y_val)))

# Train the logistic regression model using RMSProp optimizer with L1 regularization
def rmsprop_optimizer(X, y, lambd):
    m, n = X.shape
    weight = np.zeros((n, 1))
    iterations = 1000
    alpha = 0.01
    beta = 0.9 #controls the contribution of past gradients to the moving average of the squared gradients
    epsilon = 1e-8 #small constant added to the denominator to avoid division by zero
    v = np.zeros((n, 1)) #represents the moving average of the squared gradients for each weight
    for i in range(iterations):
        h = sigmoid(X.dot(weight))
        J = (-1 / m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h)) + (lambd / (2 * m)) * np.sum(np.abs(weight[1:]))
        gradient = (1 / m) * X.T.dot(h - y) + (lambd / m) * np.concatenate([[[0]], np.sign(weight[1:])], axis=0)
        v = beta * v + (1 - beta) * (gradient ** 2)
        weight -= (alpha / np.sqrt(v + epsilon)) * gradient
    return weight

#Train the logistic regression model using RMSprop optimizer
weight_rmsprop = rmsprop_optimizer(X_train, y_train, lambd=1)

# Calculate the accuracy on the validation set
y_predicted_rmsprop = predict(X_val, weight_rmsprop)
print("Accuracy with RMSProp optimizer: {:.2f}%".format(accuracy(y_predicted_rmsprop, y_val)))

# Train the logistic regression model using Adam optimizer with L1 regularization
def adam_optimizer(X, y, lambd):
    m, n = X.shape
    weight = np.zeros((n, 1))
    iterations = 1000
    alpha = 0.01
    beta1 = 0.9 # hyperparameter that controls the decay rate
    beta2 = 0.999 #hyperparameter that controls the decay rate
    epsilon = 1e-8 # to prevent division by zero
    v = np.zeros((n, 1)) #first moment
    s = np.zeros((n, 1)) ##second moment
    t = 0 #calculate the appropriate bias corrections for the moment estimates at each iteration.
    for i in range(iterations):
        t += 1 #ensures the bias corrections are accurate
        h = sigmoid(X.dot(weight))
        J = (-1 / m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h)) + (lambd / (2 * m)) * np.sum(np.abs(weight[1:]))
        grad = (1 / m) * X.T.dot(h - y) + (lambd / m) * np.concatenate([[[0]], np.sign(weight[1:])], axis=0)
        v = beta1 * v + (1 - beta1) * grad #the mean of the gradient using an exponentially decaying average
        s = beta2 * s + (1 - beta2) * (grad ** 2) #the uncentered variance of the gradient using an exponentially decaying average
        v_corrected = v / (1 - beta1 ** t) #ensures that v remains an accurate estimator of the true gradient moment, even as the algorithm runs.
        s_corrected = s / (1 - beta2 ** t) #ensures that s remains an accurate estimator of the true gradient's second moment
        weight -= (alpha / (np.sqrt(s_corrected) + epsilon)) * v_corrected
    return weight

#Train the logistic regression model using Adam optimizer
weight_adam = adam_optimizer(X_train, y_train, lambd=1)

# Calculate the accuracy on the validation set
y_predicted_adam = predict(X_val, weight_adam)
print("Accuracy with Adam optimizer: {:.2f}%".format(accuracy(y_predicted_adam, y_val)))

'''
#conclusion for each case explaining the behind reasons
L1 Regularization:
- Higher lambda values (1000) resulted in lower accuracy due to excessive regularization.
- Some regularization (lambda = 1) helped prevent overfitting and improved accuracy.

Mini-Batch Gradient Descent:
- Smaller batch sizes (10) led to higher accuracy due to faster convergence and better step sizes.
- Larger batches (100) were noisier and converged slower, resulting in lower accuracy.
- can accelerate gradient descent optimization. The ideal batch size depends on the data and model.

RMSProp Optimizer:
- Adaptively adjusted the learning rates based on gradients, leading to faster convergence and higher accuracy than basic gradient descent.

Adam Optimizer:
Used bias-corrected moment estimates to adaptively optimize the step sizes, leading to the highest accuracy of all the models.

'''